{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aef19d0-9369-45f7-81aa-d7ca27ca925d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 18:23:55.645983: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/mimer/NOBACKUP/groups/naiss2024-5-16/tloop/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/mimer/NOBACKUP/groups/naiss2024-5-16/tloop/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.11.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# standard libraries\n",
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# standard scientific libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import asarray, save, load\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# scikit-learn\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv1D, Conv2D, MaxPooling2D, Dropout, Flatten, Input, MaxPooling1D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e2bc2c",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bafc96ad-62ad-47da-834b-df3dc94e2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(metrics, num_classes, input_shape, output_bias=None, pool_size = 2):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    \n",
    "    model = Sequential([        \n",
    "        # First convolution\n",
    "        Conv1D(16, 3, strides=1, activation='relu', padding='same', # TODO set as 64?\n",
    "               input_shape = input_shape,\n",
    "               kernel_initializer = 'he_normal',\n",
    "               bias_initializer = 'zeros'),\n",
    "        MaxPooling1D(pool_size = pool_size, strides = 2), # TODO set strides?\n",
    "        Dropout(0.2),\n",
    "        Conv1D(32, 3, strides = 1, activation = 'relu', padding = 'same',\n",
    "               kernel_initializer = 'he_normal',\n",
    "               bias_initializer = 'zeros'),\n",
    "        MaxPooling1D(pool_size = pool_size, strides = 2),\n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        \n",
    "        # Neuron hidden layer\n",
    "        Dense(int(input_shape[0]/pool_size) * 32, activation = 'relu', kernel_initializer='he_normal', bias_initializer = output_bias),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Output neuron\n",
    "        # Dense(1, activation='sigmoid')  # Sigmoid for binary question. It will contain a value from 0-1 where 0 for class ('not GNRA') and 1 for the other ('GNRA')\n",
    "        Dense(num_classes, activation='softmax', bias_initializer=output_bias, kernel_initializer='glorot_uniform') # TODO categorical: Softmax for multiclass classification\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate = 1e-3),  # optimizer=RMSprop(lr=0.001),\n",
    "        loss=keras.losses.CategoricalCrossentropy(), \n",
    "        # loss=keras.losses.BinaryCrossentropy(), # TODO remove?\n",
    "        metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf891565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(DATA_DIR, NUM_CLASSES, EPOCHS = 20, BATCH_SIZE = 16):\n",
    "    DATA_DIR = Path(DATA_DIR)\n",
    "    SET_NAME = str(DATA_DIR).split(\"/\")[-1]\n",
    "    RESULTS_FILE = RESULTS_DIR / SET_NAME\n",
    "    #// RESULTS_FILE = Path(f\"results/ANN/homology_reduced/{SET_NAME}\")\n",
    "    METRICS = [ \n",
    "        keras.metrics.TruePositives(name = 'tp'), # TODO is this correct? something is wrong\n",
    "        keras.metrics.FalsePositives(name = 'fp'),\n",
    "        keras.metrics.TrueNegatives(name = 'tn'),\n",
    "        keras.metrics.FalseNegatives(name = 'fn'),\n",
    "        # keras.metrics.BinaryAccuracy(name = 'accuracy'), # TODO remove this?\n",
    "        keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "        keras.metrics.Precision(name = 'precision'),\n",
    "        keras.metrics.Recall(name = 'recall'),\n",
    "        keras.metrics.AUC(name = 'auc', curve='roc'),\n",
    "        keras.metrics.AUC(name = 'prc', curve = 'PR'), # precision-recall curve\n",
    "        tfa.metrics.F1Score(name = 'f1', num_classes = NUM_CLASSES),\n",
    "        tfa.metrics.MatthewsCorrelationCoefficient(name = 'mcc', num_classes = NUM_CLASSES)\n",
    "    ]\n",
    "\n",
    "    # Load dataset\n",
    "    \n",
    "    train_dict = np.load(DATA_DIR/\"train_matrices.npz\", allow_pickle=True)\n",
    "    x_train = np.stack(train_dict['arr_0'], axis=0)\n",
    "    y_train = np.load(DATA_DIR/\"train_labels.npy\", allow_pickle=True)\n",
    "    y_train = to_categorical(y_train, num_classes=NUM_CLASSES)\n",
    "\n",
    "    dev_dict = np.load(DATA_DIR/\"dev_matrices.npz\", allow_pickle=True)\n",
    "    x_dev = np.stack(dev_dict['arr_0'], axis=0)\n",
    "    y_dev = np.load(DATA_DIR/\"dev_labels.npy\", allow_pickle=True)\n",
    "    y_dev = to_categorical(y_dev, num_classes=NUM_CLASSES)\n",
    "\n",
    "    print(\"Training features shape:\", x_train.shape)\n",
    "    print(\"Training labels shape:\", y_train.shape)\n",
    "\n",
    "    print(\"\\nDev (validation) features shape:\", x_dev.shape)\n",
    "    print(\"Dev (validation) labels shape:\", y_dev.shape)\n",
    "\n",
    "    print(\"\\nInput shape:\", x_train.shape[1:])\n",
    "    print()\n",
    "\n",
    "    INPUT_SHAPE = x_train.shape[1:]\n",
    "\n",
    "    # Calculate class weight\n",
    "\n",
    "    # Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "    # The sum of the weights of all examples stays the same.\n",
    "    y_integers = np.argmax(y_train, axis=1)\n",
    "    class_weights = np.round(class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_integers), y=y_integers), 2)\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    # Train with class weights\n",
    "\n",
    "    model = make_model(METRICS, NUM_CLASSES, INPUT_SHAPE)\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data = (x_dev, y_dev),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        \n",
    "        # The class weights go here\n",
    "        class_weight=d_class_weights)\n",
    "    \n",
    "    # Save model\n",
    "    model.save(RESULTS_FILE, overwrite=True)\n",
    "\n",
    "    # Save history\n",
    "    with open(RESULTS_FILE/\"history\", 'wb') as file:\n",
    "        pickle.dump(history.history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5a9f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mimer/NOBACKUP/groups/naiss2024-5-16/deep-learning-RNA-structure-prediction-sam/tloop_prediction\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data_generation/training_data/homology_reduced'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#PATH = Path(\"..///\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m RESULTS_DIR \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults/ANN/homology_reduced/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mdir\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mU\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgnravall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(PATH\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mdir\u001b[39m):\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclusters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data_generation/training_data/homology_reduced'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "PATH = Path(\"../data_generation/training_data/homology_reduced/\")\n",
    "#PATH = Path(\"..///\")\n",
    "\n",
    "RESULTS_DIR = Path(\"results/ANN/homology_reduced/\")\n",
    "for dir in os.listdir(PATH):\n",
    "    if dir[-1] != \"U\" and \"gnravall\" not in dir and os.path.isdir(PATH/dir):\n",
    "        if \"clusters\" in dir:\n",
    "            pass\n",
    "            # print(dir)\n",
    "            # train(PATH/dir, 24)\n",
    "        else:\n",
    "            # pass\n",
    "            train(PATH/dir, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnabert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
