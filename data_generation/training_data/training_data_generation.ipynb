{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import utils\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "FRAGMENTS_DIR = Path('../data/fragments/')\n",
    "TRAINING_DATA_DIR = Path('homology_reduced')\n",
    "FRAGMENTS_RANGE = range(8, 25, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, train_ratio = 0.8): \n",
    "    \n",
    "    train_data, dev_data, test_data = [], [], []\n",
    "    \n",
    "    # Ensure all labels found in each set (when possible)\n",
    "    for label in list(set([i[1] for i in data])):\n",
    "        label_data = [i for i in data if i[1] == label]\n",
    "        if len(label_data) < 3: continue\n",
    "\n",
    "        random.Random(42).shuffle(label_data)\n",
    "        \n",
    "        # Add at least 1 datapoint per set\n",
    "        train_data += [label_data.pop(0)]\n",
    "        dev_data += [label_data.pop(0)]\n",
    "        test_data += [label_data.pop(0)]\n",
    "        \n",
    "        train_cutoff = int(len(label_data) * train_ratio)\n",
    "        dev_ratio = (1-train_ratio)/2\n",
    "        dev_cutoff = int(len(label_data) * (1-dev_ratio))\n",
    "        \n",
    "        train_data += label_data[:train_cutoff]\n",
    "        dev_data += label_data[train_cutoff:dev_cutoff]\n",
    "        test_data += label_data[dev_cutoff:]\n",
    "        \n",
    "    return train_data, dev_data, test_data\n",
    "\n",
    "\n",
    "def write_csv(path, data): # Data entries should be formatted as (sequence, label) tuple\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(\"sequence,label\\n\")\n",
    "        for entry in data:\n",
    "            f.write(f\"{entry[0]},{entry[1]}\\n\")\n",
    "\n",
    "\n",
    "def encode_sequence(sequence, residue_map = {'A':0,'U':1,'C':2,'G':3,'I':4,'N':5}):\n",
    "    seq_array = np.array([residue_map[i] for i in sequence], dtype=int)\n",
    "    encoded_array = np.zeros((seq_array.size, len(set(residue_map.values()))), dtype=int)\n",
    "    encoded_array[np.arange(seq_array.size), seq_array] = 1\n",
    "    return encoded_array\n",
    "\n",
    "\n",
    "def write_feature_matrix(path, data):\n",
    "    encoded_matrices = [encode_sequence(i[0]) for i in data]\n",
    "    np.savez(path, np.array(encoded_matrices))\n",
    "\n",
    "\n",
    "def write_label(path, data):\n",
    "    labels = [i[1] for i in data]\n",
    "    np.save(path, labels)\n",
    "\n",
    "\n",
    "def write_label_matrix(path, data):\n",
    "    enc = OneHotEncoder(categories=[range(45)])\n",
    "    labels = [enc.fit_transform(np.array(i[1]).reshape(-1,1)) for i in data]\n",
    "    np.savez(path, np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fragment data\n",
    "for i in FRAGMENTS_RANGE:\n",
    "    globals()[f\"fragments_{i}\"] = utils.load(FRAGMENTS_DIR/f\"fragments_{i}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating gnra 8\n",
      "Generating gnra 10\n",
      "Generating gnra 12\n",
      "Generating gnra 14\n",
      "Generating gnra 16\n",
      "Generating gnra 18\n",
      "Generating gnra 20\n",
      "Generating gnra 22\n",
      "Generating gnra 24\n",
      "Generating tloop 8\n",
      "Generating tloop 10\n",
      "Generating tloop 12\n",
      "Generating tloop 14\n",
      "Generating tloop 16\n",
      "Generating tloop 18\n",
      "Generating tloop 20\n",
      "Generating tloop 22\n",
      "Generating tloop 24\n",
      "Generating clusters 8\n",
      "Generating clusters 10\n",
      "Generating clusters 12\n",
      "Generating clusters 14\n",
      "Generating clusters 16\n",
      "Generating clusters 18\n",
      "Generating clusters 20\n",
      "Generating clusters 22\n",
      "Generating clusters 24\n"
     ]
    }
   ],
   "source": [
    "for task in [\"gnra\", \"tloop\", \"clusters\"]: # \"gnra\", \"tloop\", \"clusters\"\n",
    "    for fragment_length in [8, 10, 12, 14, 16, 18, 20, 22, 24]: # 8, 10, 12, 14, 16, 18, 20, 22, 24\n",
    "    #// for train_ratio in [0.8]:\n",
    "    #// for nucleotides in [\"T\", \"U\"]:\n",
    "        print(f\"Generating {task} {fragment_length}\")\n",
    "        \n",
    "        # Create/replace existing data directory\n",
    "        data_dir = TRAINING_DATA_DIR / f\"{task}_{fragment_length}/\" #//_{int(train_ratio*100)}_{nucleotides}\n",
    "        if data_dir.exists():\n",
    "            shutil.rmtree(data_dir)\n",
    "        data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Retrieve fragments of corresponding length\n",
    "        fragments = globals()[f'fragments_{fragment_length}']\n",
    "        \n",
    "        # Reformat fragment data depending on the task\n",
    "        match task:\n",
    "            case \"gnra\":\n",
    "                data = [i for i in fragments if i.clust_id != 0]\n",
    "                data = [(i.res_seq, 1) if i.clust_id == 1 else (i.res_seq, 0) for i in data]\n",
    "            case \"tloop\":\n",
    "                data = [(i.res_seq, 0) if i.clust_id == 0 else (i.res_seq, 1) for i in fragments]\n",
    "            case \"clusters\":\n",
    "                folds = {\"Decoy\": [0], \"GNRA\": [1, 3, 6, 9, 25, 26, 36, 40], \"UNCG\": [2, 5, 37, 44], \"U-TURN\": [4], \"7\": [7], \"8\": [8], \"10\": [10], \"4-Stack\": [11], \"12/34\": [12, 34], \"13/20\": [13, 20], \"14/19\": [14, 19], \"15\": [15], \"16\": [16], \"17\": [17], \"18\": [18], \"RNYA\": [21], \"22/32/43\": [22, 32, 43], \"23\": [23], \"24\": [24], \"27\": [27], \"28\": [28], \"29\": [29], \"GGUG\": [30], \"31\": [31], \"33\": [33], \"CUUG\": [35, 38], \"AGNN\": [39], \"41\": [41], \"42\": [42]} # Clusters corresponding to the same fold, as defined by Bottaro\n",
    "                data = []\n",
    "                for idx, (fold, clust_ids) in enumerate(folds.items()):\n",
    "                    for clust_id in clust_ids:\n",
    "                        data += [(i.res_seq, idx) for i in fragments if i.clust_id == clust_id]\n",
    "                #// Clusters 23 - 44 lumped into one group under label \"23\"\n",
    "                #// data = [(i.res_seq, i.clust_id) if i.clust_id not in range(23, 45) else (i.res_seq, 23) for i in fragments]\n",
    "            #// case \"gnravall\":\n",
    "            #//     data = [i for i in fragments if i.clust_id != 0]\n",
    "            #//     data = [(i.res_seq, 1) if i.clust_id == 1 else (i.res_seq, 0) for i in fragments]\n",
    "        \n",
    "        # Homology reduction\n",
    "        data = list(set(data))\n",
    "        \n",
    "        #// # Replace T with U\n",
    "        #// if nucleotides == \"T\":\n",
    "        #//     data = [(i.replace(\"U\",\"T\"), j) for i, j in data]\n",
    "\n",
    "        # Split data into train, dev, and test sets\n",
    "        train_data, dev_data, test_data = split_data(data) #//train_ratio\n",
    "        \n",
    "        # Save datasets\n",
    "        write_csv(data_dir/\"train.csv\", train_data)\n",
    "        write_feature_matrix(data_dir/\"train_matrices.npz\", train_data)\n",
    "        write_label(data_dir/\"train_labels.npy\", train_data)\n",
    "\n",
    "        write_csv(data_dir/\"dev.csv\", dev_data)\n",
    "        write_feature_matrix(data_dir/\"dev_matrices.npz\", dev_data)\n",
    "        write_label(data_dir/\"dev_labels.npy\", dev_data)\n",
    "\n",
    "        write_csv(data_dir/\"test.csv\", test_data)\n",
    "        write_feature_matrix(data_dir/\"test_matrices.npz\", test_data)\n",
    "        write_label(data_dir/\"test_labels.npy\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// # Load fragment data (multiple clusters)\n",
    "#//fragments_multi_8 = utils.load(fragments_dir/'fragments_multi_8.pickle')\n",
    "#//fragments_multi_24 = utils.load(fragments_dir/'fragments_multi_24.pickle')\n",
    "#//fragments_multi_48 = utils.load(fragments_dir/'fragments_multi_48.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// for fragment_length in [8, 24, 48]: # 8, 10, 12, 14, 16, 18, 20, 22, 24\n",
    "#//\n",
    "#//    # Create/replace existing data directory\n",
    "#//    data_dir = training_data_dir / f\"multi_{fragment_length}/\"\n",
    "#//    if data_dir.exists():\n",
    "#//        shutil.rmtree(data_dir)\n",
    "#//    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "#//    \n",
    "#//    # Retrieve fragments of corresponding length\n",
    "#//    fragments = globals()[f'fragments_multi_{fragment_length}']\n",
    "#//    \n",
    "#//    data = [(i.res_seq, i.clust_id) for i in fragments]\n",
    "#//    \n",
    "#//    # Homology reduction\n",
    "#//    data = list(set(data))\n",
    "#//\n",
    "#//    # Split data into train, dev, and test sets\n",
    "#//    train_data, dev_data, test_data = split_data(data)\n",
    "#//    train_data = [(i[0], np.array(i[1])) for i in train_data]\n",
    "#//    dev_data = [(i[0], np.array(i[1])) for i in dev_data]\n",
    "#//    test_data = [(i[0], np.array(i[1])) for i in test_data]\n",
    "#//    \n",
    "#//    # Save datasets\n",
    "#//    write_csv(data_dir/\"train.csv\", train_data)\n",
    "#//    write_feature_matrix(data_dir/\"train_matrices.npz\", train_data)\n",
    "#//    write_label_matrix(data_dir/\"train_labels.npz\", train_data)\n",
    "#//\n",
    "#//    write_csv(data_dir/\"dev.csv\", dev_data)\n",
    "#//    write_feature_matrix(data_dir/\"dev_matrices.npz\", dev_data)\n",
    "#//    write_label_matrix(data_dir/\"dev_labels.npz\", dev_data)\n",
    "#//\n",
    "#//    write_csv(data_dir/\"test.csv\", test_data)\n",
    "#//    write_feature_matrix(data_dir/\"test_matrices.npz\", test_data)\n",
    "#//    write_label_matrix(data_dir/\"test_labels.npz\", test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
