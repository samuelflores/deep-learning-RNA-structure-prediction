{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import utils\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "FRAGMENTS_DIR = Path('../data/fragments/')\n",
    "TRAINING_DATA_DIR = Path('811')\n",
    "FRAGMENTS_RANGE = range(8, 25, 2)\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, train_ratio = 0.8):\n",
    "    \n",
    "    train_data, dev_data, test_data = [], [], []\n",
    "    \n",
    "    # Ensure all labels found in each set (when possible)\n",
    "    for label in list(set([i[1] for i in data])):\n",
    "        label_data = [i for i in data if i[1] == label]\n",
    "        if len(label_data) < 3: continue\n",
    "        \n",
    "        random.Random(SEED).shuffle(label_data)\n",
    "        \n",
    "        # Add at least 1 datapoint per set\n",
    "        train_data += [label_data.pop(0)]\n",
    "        dev_data += [label_data.pop(0)]\n",
    "        test_data += [label_data.pop(0)]\n",
    "        \n",
    "        train_cutoff = int(len(label_data) * train_ratio)\n",
    "        dev_ratio = (1-train_ratio)/2\n",
    "        dev_cutoff = int(len(label_data) * (1-dev_ratio))\n",
    "        \n",
    "        train_data += label_data[:train_cutoff]\n",
    "        dev_data += label_data[train_cutoff:dev_cutoff]\n",
    "        test_data += label_data[dev_cutoff:]\n",
    "        \n",
    "    return train_data, dev_data, test_data\n",
    "\n",
    "\n",
    "def write_csv(path, data): # Data entries should be formatted as (sequence, label) tuple\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(\"sequence,label\\n\")\n",
    "        for entry in data:\n",
    "            f.write(f\"{entry[0]},{entry[1]}\\n\")\n",
    "\n",
    "\n",
    "def encode_sequence(sequence, residue_map = {'A':0,'U':1,'C':2,'G':3,'I':4,'N':5}):\n",
    "    seq_array = np.array([residue_map[i] for i in sequence], dtype=int)\n",
    "    encoded_array = np.zeros((seq_array.size, len(set(residue_map.values()))), dtype=int)\n",
    "    encoded_array[np.arange(seq_array.size), seq_array] = 1\n",
    "    return encoded_array\n",
    "\n",
    "\n",
    "def write_feature_matrices(path, data):\n",
    "    encoded_matrices = [encode_sequence(i[0]) for i in data]\n",
    "    np.savez(path, np.array(encoded_matrices))\n",
    "\n",
    "\n",
    "def write_labels(path, data):\n",
    "    labels = [i[1] for i in data]\n",
    "    np.save(path, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fragment data\n",
    "for i in FRAGMENTS_RANGE:\n",
    "    globals()[f\"fragments_{i}\"] = utils.load(FRAGMENTS_DIR/f\"fragments_{i}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating gnravr 8\n",
      "Generating gnravr 10\n",
      "Generating gnravr 12\n",
      "Generating gnravr 14\n",
      "Generating gnravr 16\n",
      "Generating gnravr 18\n",
      "Generating gnravr 20\n",
      "Generating gnravr 22\n",
      "Generating gnravr 24\n",
      "Generating gnralikevr 8\n",
      "Generating gnralikevr 10\n",
      "Generating gnralikevr 12\n",
      "Generating gnralikevr 14\n",
      "Generating gnralikevr 16\n",
      "Generating gnralikevr 18\n",
      "Generating gnralikevr 20\n",
      "Generating gnralikevr 22\n",
      "Generating gnralikevr 24\n",
      "Generating uncgvr 8\n",
      "Generating uncgvr 10\n",
      "Generating uncgvr 12\n",
      "Generating uncgvr 14\n",
      "Generating uncgvr 16\n",
      "Generating uncgvr 18\n",
      "Generating uncgvr 20\n",
      "Generating uncgvr 22\n",
      "Generating uncgvr 24\n",
      "Generating uncglikevr 8\n",
      "Generating uncglikevr 10\n",
      "Generating uncglikevr 12\n",
      "Generating uncglikevr 14\n",
      "Generating uncglikevr 16\n",
      "Generating uncglikevr 18\n",
      "Generating uncglikevr 20\n",
      "Generating uncglikevr 22\n",
      "Generating uncglikevr 24\n"
     ]
    }
   ],
   "source": [
    "for task in [\"gnravr\", \"gnralikevr\", \"uncgvr\", \"uncglikevr\"]:\n",
    "    for fragment_length in [8, 10, 12, 14, 16, 18, 20, 22, 24]: # 8, 10, 12, 14, 16, 18, 20, 22, 24\n",
    "        print(f\"Generating {task} {fragment_length}\")\n",
    "        \n",
    "        # Create/replace existing data directory\n",
    "        data_dir = TRAINING_DATA_DIR / f\"{task}_{fragment_length}/\"\n",
    "        if data_dir.exists():\n",
    "            shutil.rmtree(data_dir)\n",
    "        data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Retrieve fragments of corresponding length\n",
    "        fragments = globals()[f'fragments_{fragment_length}']\n",
    "        \n",
    "        # Reformat fragment data depending on the task\n",
    "        match task:\n",
    "            case \"gnra\":\n",
    "                data = [i for i in fragments if i.clust_id != 0]\n",
    "                data = [(i.res_seq, 1) if i.clust_id == 1 else (i.res_seq, 0) for i in data]\n",
    "            case \"gnravr\":\n",
    "                data = [(i.res_seq, 1) if i.clust_id == 1 else (i.res_seq, 0) for i in fragments]\n",
    "            case \"gnralike\":\n",
    "                data = [i for i in fragments if i.clust_id != 0]\n",
    "                data = [(i.res_seq, 1) if i.clust_id in [1, 3, 6, 9, 25, 26, 36, 40] else (i.res_seq, 0) for i in data]\n",
    "            case \"gnralikevr\":\n",
    "                data = [(i.res_seq, 1) if i.clust_id in [1, 3, 6, 9, 25, 26, 36, 40] else (i.res_seq, 0) for i in fragments]\n",
    "            case \"uncg\":\n",
    "                data = [i for i in fragments if i.clust_id != 0]\n",
    "                data = [(i.res_seq, 1) if i.clust_id == 2 else (i.res_seq, 0) for i in data]\n",
    "            case \"uncgvr\":\n",
    "                data = [(i.res_seq, 1) if i.clust_id == 2 else (i.res_seq, 0) for i in fragments]\n",
    "            case \"uncglike\":\n",
    "                data = [i for i in fragments if i.clust_id != 0]\n",
    "                data = [(i.res_seq, 1) if i.clust_id in [2, 5, 37, 44] else (i.res_seq, 0) for i in data]\n",
    "            case \"uncglikevr\":\n",
    "                data = [(i.res_seq, 1) if i.clust_id in [2, 5, 37, 44] else (i.res_seq, 0) for i in fragments]\n",
    "            case \"tloop\":\n",
    "                data = [(i.res_seq, 0) if i.clust_id == 0 else (i.res_seq, 1) for i in fragments]\n",
    "            case \"folds\":\n",
    "                folds = {\"Decoy\": [0], \"GNRA\": [1, 3, 6, 9, 25, 26, 36, 40], \"UNCG\": [2, 5, 37, 44], \"U-TURN\": [4], \"7\": [7], \"8\": [8], \"10\": [10], \"4-Stack\": [11], \"12/34\": [12, 34], \"13/20\": [13, 20], \"14/19\": [14, 19], \"15\": [15], \"16\": [16], \"17\": [17], \"18\": [18], \"RNYA\": [21], \"22/32/43\": [22, 32, 43], \"23\": [23], \"24\": [24], \"27\": [27], \"28\": [28], \"29\": [29], \"GGUG\": [30], \"31\": [31], \"33\": [33], \"CUUG\": [35, 38], \"AGNN\": [39], \"41\": [41], \"42\": [42]} # Clusters corresponding to the same fold, as defined by Bottaro\n",
    "                data = []\n",
    "                for idx, (fold, clust_ids) in enumerate(folds.items()):\n",
    "                    for clust_id in clust_ids:\n",
    "                        data += [(i.res_seq, idx) for i in fragments if i.clust_id == clust_id]\n",
    "        \n",
    "        # Homology reduction\n",
    "        data = list(set(data))\n",
    "        \n",
    "        # Split data into train, dev, and test sets\n",
    "        train_data, dev_data, test_data = split_data(data)\n",
    "        \n",
    "        # Save datasets\n",
    "        write_csv(data_dir/\"train.csv\", train_data)\n",
    "        write_feature_matrices(data_dir/\"train_matrices.npz\", train_data)\n",
    "        write_labels(data_dir/\"train_labels.npy\", train_data)\n",
    "        \n",
    "        write_csv(data_dir/\"dev.csv\", dev_data)\n",
    "        write_feature_matrices(data_dir/\"dev_matrices.npz\", dev_data)\n",
    "        write_labels(data_dir/\"dev_labels.npy\", dev_data)\n",
    "        \n",
    "        write_csv(data_dir/\"test.csv\", test_data)\n",
    "        write_feature_matrices(data_dir/\"test_matrices.npz\", test_data)\n",
    "        write_labels(data_dir/\"test_labels.npy\", test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
