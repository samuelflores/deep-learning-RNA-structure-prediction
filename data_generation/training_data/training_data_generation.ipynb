{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import utils\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "fragments_dir = Path('../data/fragments/')\n",
    "training_data_dir = Path('sample_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, train_ratio = 0.8): \n",
    "    \n",
    "    train_data, dev_data, test_data = [], [], []\n",
    "    \n",
    "    # Ensure all labels found in each set (when possible)\n",
    "    for label in list(set([i[1] for i in data])):\n",
    "        label_data = [i for i in data if i[1] == label]\n",
    "        if len(label_data) < 3: continue\n",
    "\n",
    "        random.shuffle(label_data)\n",
    "\n",
    "        # Add at least 1 datapoint per set\n",
    "        train_data += [label_data.pop(0)]\n",
    "        dev_data += [label_data.pop(0)]\n",
    "        test_data += [label_data.pop(0)]\n",
    "        \n",
    "        train_cutoff = int(len(label_data) * train_ratio)\n",
    "        dev_ratio = (1-train_ratio)/2\n",
    "        dev_cutoff = int(len(label_data) * (1-dev_ratio))\n",
    "        \n",
    "        train_data += label_data[:train_cutoff]\n",
    "        dev_data += label_data[train_cutoff:dev_cutoff]\n",
    "        test_data += label_data[dev_cutoff:]\n",
    "        \n",
    "    return train_data, dev_data, test_data\n",
    "\n",
    "\n",
    "def write_csv(path, data): # Data entries should be formatted as (sequence, label) tuple\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(\"sequence,label\\n\")\n",
    "        for entry in data:\n",
    "            f.write(f\"{entry[0]},{entry[1]}\\n\")\n",
    "\n",
    "\n",
    "def encode_sequence(sequence, residue_map = {'A':0,'U':1,'T':1,'C':2,'G':3,'I':4}): # TODO should I be included?\n",
    "    seq_array = np.array([residue_map[i] for i in sequence if i in residue_map.keys()])\n",
    "    encoded_array = np.zeros((seq_array.size, 5), dtype=int)\n",
    "    encoded_array[np.arange(seq_array.size), seq_array] = 1\n",
    "    return encoded_array\n",
    "\n",
    "\n",
    "def write_matrix(path, data):\n",
    "    encoded_matrices = [encode_sequence(i[0]) for i in data]\n",
    "    np.savez(path, np.array(encoded_matrices))\n",
    "\n",
    "\n",
    "def write_label(path, data):\n",
    "    labels = [i[1] for i in data]\n",
    "    np.save(path, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fragment data (single cluster)\n",
    "fragments_8 = utils.load(fragments_dir/'fragments_8_filtered.pickle')\n",
    "fragments_10 = utils.load(fragments_dir/'fragments_10_filtered.pickle')\n",
    "fragments_12 = utils.load(fragments_dir/'fragments_12_filtered.pickle')\n",
    "fragments_14 = utils.load(fragments_dir/'fragments_14_filtered.pickle')\n",
    "fragments_16 = utils.load(fragments_dir/'fragments_16_filtered.pickle')\n",
    "fragments_18 = utils.load(fragments_dir/'fragments_18_filtered.pickle')\n",
    "fragments_20 = utils.load(fragments_dir/'fragments_20_filtered.pickle')\n",
    "fragments_22 = utils.load(fragments_dir/'fragments_22_filtered.pickle')\n",
    "fragments_24 = utils.load(fragments_dir/'fragments_24_filtered.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in [\"clusters\"]: # \"gnra\", \"clusters\", \"tloop\"\n",
    "    for fragment_length in [8]: # 8, 10, 12, 14, 16, 18, 20, 22, 24\n",
    "    #// for train_ratio in [0.8]:\n",
    "    #// for nucleotides in [\"T\", \"U\"]:\n",
    "\n",
    "        # Create/replace existing data directory\n",
    "        data_dir = training_data_dir / f\"{task}_{fragment_length}/\" #//_{int(train_ratio*100)}_{nucleotides}\n",
    "        if data_dir.exists():\n",
    "            shutil.rmtree(data_dir)\n",
    "        data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Retrieve fragments of corresponding length\n",
    "        fragments = globals()[f'fragments_{fragment_length}']\n",
    "\n",
    "        # Reformat fragment data depending on the task\n",
    "        match task:\n",
    "            case \"gnra\":\n",
    "                data = [i for i in fragments if i.clust_id != 0]\n",
    "                data = [(i.res_seq, 1) if i.clust_id == 1 else (i.res_seq, 0) for i in data]\n",
    "            case \"tloop\":\n",
    "                data = [(i.res_seq, 0) if i.clust_id == 0 else (i.res_seq, 1) for i in fragments]\n",
    "            case \"clusters\":\n",
    "                folds = {\"GNRA\": [1, 3, 6, 9, 25, 26, 36, 40], \"UNCG\": [2, 5, 37, 44], \"U-TURN\": [4], \"7\": [7], \"8\": [8], \"10\": [10], \"4-Stack\": [11], \"12/34\": [12, 34], \"13/20\": [13, 20], \"14/19\": [14, 19], \"15\": [15], \"16\": [16], \"17\": [17], \"18\": [18], \"RNYA\": [21], \"22/32/43\": [22, 32, 43], \"23\": [23], \"24\": [24], \"27\": [27], \"28\": [28], \"29\": [29], \"GGUG\": [30], \"31\": [31], \"33\": [33], \"CUUG\": [35, 38], \"AGNN\": [39], \"41\": [41], \"42\": [42]} # Clusters corresponding to the same fold, as defined by Bottaro\n",
    "                data = []\n",
    "                for idx, (fold, clust_ids) in enumerate(folds.items()):\n",
    "                    for clust_id in clust_ids:\n",
    "                        data += [(i.res_seq, idx) for i in fragments if i.clust_id == clust_id]\n",
    "                #// Clusters 23 - 44 lumped into one group under label \"23\"\n",
    "                #// data = [(i.res_seq, i.clust_id) if i.clust_id not in range(23, 45) else (i.res_seq, 23) for i in fragments]\n",
    "            #// case \"gnravall\":\n",
    "            #//     data = [i for i in fragments if i.clust_id != 0]\n",
    "            #//     data = [(i.res_seq, 1) if i.clust_id == 1 else (i.res_seq, 0) for i in fragments]\n",
    "        \n",
    "        # Homology reduction\n",
    "        data = list(set(data))\n",
    "        \n",
    "        #// # Replace T with U\n",
    "        #// if nucleotides == \"T\":\n",
    "        #//     data = [(i.replace(\"U\",\"T\"), j) for i, j in data]\n",
    "\n",
    "        # Split data into train, dev, and test sets\n",
    "        train_data, dev_data, test_data = split_data(data) #//train_ratio\n",
    "        \n",
    "        # Save datasets\n",
    "        write_csv(data_dir/\"train.csv\", train_data)\n",
    "        write_matrix(data_dir/\"train_matrices.npz\", train_data)\n",
    "        write_label(data_dir/\"train_labels.npy\", train_data)\n",
    "\n",
    "        write_csv(data_dir/\"dev.csv\", dev_data)\n",
    "        write_matrix(data_dir/\"dev_matrices.npz\", dev_data)\n",
    "        write_label(data_dir/\"dev_labels.npy\", dev_data)\n",
    "\n",
    "        write_csv(data_dir/\"test.csv\", test_data)\n",
    "        write_matrix(data_dir/\"test_matrices.npz\", test_data)\n",
    "        write_label(data_dir/\"test_labels.npy\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fragment data (multiple clusters)\n",
    "fragments_multi_8 = utils.load(fragments_dir/'fragments_multi_8_filtered.pickle')\n",
    "fragments_multi_24 = utils.load(fragments_dir/'fragments_multi_24_filtered.pickle')\n",
    "fragments_multi_48 = utils.load(fragments_dir/'fragments_multi_48_filtered.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fragment_length in [8, 24, 48]: # 8, 10, 12, 14, 16, 18, 20, 22, 24\n",
    "\n",
    "    # Create/replace existing data directory\n",
    "    data_dir = training_data_dir / f\"multi_{fragment_length}/\"\n",
    "    if data_dir.exists():\n",
    "        shutil.rmtree(data_dir)\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Retrieve fragments of corresponding length\n",
    "    fragments = globals()[f'fragments_multi_{fragment_length}']\n",
    "    \n",
    "    data = [(i.res_seq, i.clust_id) for i in fragments]\n",
    "    \n",
    "    # Homology reduction\n",
    "    data = list(set(data))\n",
    "    \n",
    "    # Split data into train, dev, and test sets\n",
    "    train_data, dev_data, test_data = split_data(data)\n",
    "    \n",
    "    # Save datasets\n",
    "    write_csv(data_dir/\"train.csv\", train_data)\n",
    "    write_matrix(data_dir/\"train_matrices.npz\", train_data)\n",
    "    write_matrix(data_dir/\"train_labels.npz\", train_data)\n",
    "\n",
    "    write_csv(data_dir/\"dev.csv\", dev_data)\n",
    "    write_matrix(data_dir/\"dev_matrices.npz\", dev_data)\n",
    "    write_matrix(data_dir/\"dev_labels.npz\", dev_data)\n",
    "\n",
    "    write_csv(data_dir/\"test.csv\", test_data)\n",
    "    write_matrix(data_dir/\"test_matrices.npz\", test_data)\n",
    "    write_matrix(data_dir/\"test_labels.npz\", test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
